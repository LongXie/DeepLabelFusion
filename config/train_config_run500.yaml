# configuration for generating training patches
reorganize:
  # Number of samples per atlas target pair for training
  num_pos_train: 10
  num_neg_train: 2
  # Number of samples per atlas target pair for validation
  num_pos_val: 2
  num_neg_val: 0
  # Patch size 
  patch_size: [72, 72, 72]
  test_patch_size: [72, 72, 72]
  test_patch_spacing: [36, 36, 36]
  # Number of times of pre augmentation
  num_aug: 0
  perc_aug: 0.4
  #perc_deform: 0.5
  # Number of voxels crobed
  patch_crop_size: 4
  # Whether to include atlases
  with_atlas: True
  # Sample subset of atlases for training
  num_sel_atlas: 6
  num_atlas_aug: 3

# use a fixed random seed to guarantee that when you run the code twice you will get the same outcome
# dont use
# manual_seed: 0

# dataset configuration
dataset:
#  train_val_csv: '/home/longxie/DeepLabelFusion/PredictSegFeatures_Sadhana/fold_0/data/split_target.csv'


# model configuration
model:
  # model class
  name: LabelfusionUNet3DLabelWeightFineTunningUNetPerChannelSkipMaskCoordMapsBothDeepSupervision4LevelReLu
  # number of input channels to the model
  in_channels: 4
  # number of output channels
  out_channels: 15
  # initial feature number
  init_feature_number: 32
  # number of levels
  num_levels: 3
  # batch normalization method
  norm: 'bn'
  # intermediate activation
  inter_sigmoid: True
  # mean smoothing radius
  rs: 0
  # fine tunning layers
  fine_tunning_layers: True
  num_tunning_levels: 4
  init_tunning_feature_number: 32
  # final activation
  final_sigmoid: True
  # number of label classes
  nclass: 15
  # dropouts
  down_dropout_rate: 0.0
  up_dropout_rate: 0.0
  tunning_dropout_rate: 0.0
  

# trainer configuration
trainer:
  # trainer name
  name: DeepLabelFusionTrainer
  # path to latest checkpoint; if provided the training will be resumed from that checkpoint
  resume: null
  # how many iterations between validations
  validate_after_iters: 300
  # how many iterations between tensorboard logging
  log_after_iters: 10
  # max number of epochs
  max_num_epochs: 30
  # max number of iterations
  max_num_iterations: 100000
  # model with higher eval score is considered better
  eval_score_higher_is_better: True
  # deep supervision
  deep_supervision: True
  deep_supervision_weights: [0.1, 0.2, 0.5, 1.0]
  # multimodal settings
  #multimodal_augment: True
  #multimodal_augment_rate: 0.5 
  

# optimizer configuration
optimizer:
  # initial learning rate
  learning_rate: 0.0005
  # weight decay
  weight_decay: 0.0001


# loss function configuration
loss:
  # loss function to be used during training (use weight = sqrt weight)
  name: GeneralizedDiceLossWeighted
  # A manual rescaling weight given to each class.
  weight: [1,1,1,1,1,1,0.1,0.1,1,1,1,1,1,0.1,0.1]
  # a target value that is ignored and does not contribute to the input gradient
  ignore_index: null
  # sigmoid has already been done in the model (final_sigmoind = True in model setting)
  sigmoid_normalization: False
  

# evaluation metric configuration
eval_metric:
  name: PercCorrectLabel
  # a target label that is ignored during metric evaluation
  ignore_index: null


lr_scheduler:
  name: MultiStepLR
  milestones: [15, 20, 25]
  gamma: 0.2


# data loaders configuration
loaders:
  # how many subprocesses to use for data loading
  num_workers: 2
  # training and validation batch size
  train_batch_size: 1
  val_batch_size: 3
  test_batch_size: 1
  # data transformations/augmentations
  transformer:
    train:
      raw:
        - name: MONAI_ToTensor
      label:
        - name: MONAI_ToTensor
    test:
      raw:
        - name: MONAI_ToTensor
      label:
        - name: MONAI_ToTensor
